# -*- coding: utf-8 -*-
"""AIES Experiment 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m3z7EZXRMk_JGCNOpN5BDCJA0TbojAV_
"""

Name:-Sudhanshu Kerle
Class:-B.Tech B DIVISION

!pip install fairlearn

"""- This installs the fairlearn library, which helps in assessing and mitigating unfairness in machine learning models.

"""

import pandas as pd
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from fairlearn.metrics import MetricFrame, selection_rate

"""- Imports necessary Python libraries for data handling, modeling, evaluation, and fairness analysis."""

data= pd.read_csv("/content/adult.csv")

"""- Loads the Adult Census Income dataset, commonly used for fairness-related case studies."""

print(data.columns.values)

"""- Prints the column names to understand available features.

"""

# Preprocess
data = data[['age', 'educational-num', 'hours-per-week', 'gender', 'income']]
data = pd.get_dummies(data, drop_first=True)
#data = pd.get_dummies(data)

"""- Filters the dataset to 5 key features.

- Applies one-hot encoding to convert categorical variables like gender and income into numeric binary form.

- drop_first=True avoids multicollinearity by dropping one level of each categorical variable.
"""

print(data.columns.values)

X = data.drop('income_>50K', axis=1)
y = data['income_>50K']

"""- X: Feature matrix (independent variables)

- y: Label vector (1 if income >50K, else 0)
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

"""- Splits the data into 70% training and 30% testing.

- Initializes and trains a logistic regression classifier on the training data.

- Predicts income category on the test set and prints the predictions.
"""

print(y_pred)

# Fairness evaluation
sex = X_test['gender_Male']
metric_frame = MetricFrame(metrics=selection_rate,
                           y_true=y_test,
                           y_pred=y_pred,
                           sensitive_features=sex)
print("Selection Rates by Gender:\n", metric_frame.by_group)

"""- Calculates selection rate (fraction of positive predictions) for each gender.

- sensitive_features=sex splits the metric by gender.


"""

#Visualise this result
import matplotlib.pyplot as plt
import pandas as pd

# Selection rates from Fairlearn output
selection_rates = {
    'Female': 0.013193,
    'Male': 0.200775
}

# Convert to DataFrame
df = pd.DataFrame(list(selection_rates.items()), columns=["Gender", "Selection Rate"])

# Plot
plt.figure(figsize=(6, 4))
bars = plt.bar(df["Gender"], df["Selection Rate"], color=["#ff9999", "#66b3ff"])
plt.title("Selection Rates by Gender")
plt.ylabel("Selection Rate")
plt.ylim(0, 0.25)
plt.grid(axis='y', linestyle='--', alpha=0.6)

# Add percentage labels above bars
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.005, f"{yval:.2%}", ha='center', va='bottom')

plt.tight_layout()
plt.show()

"""- Visualizes the disparity in selection rates using a bar chart.

- Male selection rate is significantly higher â†’ indicates bias.

#  Re-train Using Fairness Constraints
"""

from fairlearn.reductions import ExponentiatedGradient, DemographicParity
from fairlearn.metrics import MetricFrame, selection_rate

"""- Imports tools to train models under fairness constraints.

- ExponentiatedGradient is an in-processing algorithm.

- DemographicParity ensures prediction rate is equal across groups.
"""

data = pd.read_csv("adult.csv")
print(data.columns.values)

data = data[['age', 'educational-num', 'hours-per-week', 'gender', 'income']]
data = pd.get_dummies(data, drop_first=True)

"""- Repeats earlier data preprocessing to start fresh before fairness-aware training.


"""

# Prepare X and y
X = data.drop('income_>50K', axis=1)
y = data['income_>50K']
sensitive_feature = X['gender_Male']

"""- sensitive_feature: Binary indicator for gender (1=Male, 0=Female)"""

X_train, X_test, y_train, y_test, sf_train, sf_test = train_test_split(
    X, y, sensitive_feature, test_size=0.3, random_state=42
)

"""- Splits both the data and sensitive attribute for fairness-aware training/testing."""

# Define base estimator
estimator = LogisticRegression(max_iter=1000)

"""- Defines logistic regression as the base estimator."""

# Apply fairness constraint: Demographic Parity
fair_model = ExponentiatedGradient(
    estimator,
    constraints=DemographicParity(),
    eps=0.01  # fairness tolerance
)

"""- Wraps the model with a fairness constraint, trying to minimize accuracy loss while meeting demographic parity (equal selection rates)."""

# Fit fair model
fair_model.fit(X_train, y_train, sensitive_features=sf_train)

"""- Trains the model using the fairness-aware algorithm.


"""

# Predict
y_pred = fair_model.predict(X_test)

"""- Makes predictions with the fairness-constrained model."""

# Evaluate fairness and accuracy
metric_frame = MetricFrame(
    metrics={
        "Selection Rate": selection_rate,
        "Accuracy": accuracy_score
    },
    y_true=y_test,
    y_pred=y_pred,
    sensitive_features=sf_test
)

"""- Evaluates both selection rate and accuracy, broken down by gender"""

print("Fair Model Evaluation by Gender:\n")
print(metric_frame.by_group)
print("\nOverall Accuracy:", accuracy_score(y_test, y_pred))

"""- Prints selection rate and accuracy for both males and females.

- Also prints the overall accuracy of the fair model.
"""

